from typing import Tuple

import numpy as np
from gym.utils import EzPickle

from rl_squared.envs.ant.base_ant_env import BaseAntEnv


class AntTargetPositionEnv(BaseAntEnv, EzPickle):
    def __init__(
        self,
        max_episode_steps: int = 100,
        min_position: float = 0.0,
        max_position: float = 3.0,
        seed: int = None,
    ):
        """
        Ant environment with target position.

        The code is adapted from https://github.com/cbfinn/maml_rl

        The ant follows the dynamics from MuJoCo [1], and receives at each time step a reward composed of a control cost, a
        contact cost, a survival reward, and a penalty equal to its L1 distance to the target position. The tasks are
        generated by sampling the target positions from the uniform distribution on [-3, 3]^2.

        [1] Emanuel Todorov, Tom Erez, Yuval Tassa, "MuJoCo: A physics engine for model-based control", 2012
            (https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf)

        Args:
            max_episode_steps (int): Maximum number of steps before the episode is terminated.
            min_position (float): Lowerbound for target position (for x & y axis).
            max_position (float): Upperbound for target position (for x & y axis).
            seed (int): Random seed.
        """
        self._max_episode_steps = max_episode_steps
        self._elapsed_steps = 0

        self._min_position = min_position
        self._max_position = max_position

        # set a stub, sample later.
        self._target_position = np.random.uniform(
            self._min_position, self._max_position, size=2
        )

        BaseAntEnv.__init__(self)
        EzPickle.__init__(self)

        # sample
        self.seed(seed)
        self.sample_task()
        pass

    def step(self, action: np.ndarray) -> Tuple:
        """
        Take a step in the environment.

        Args:
            action (np.ndarray): Action to be taken in the environment.

        Returns:
            Tuple
        """
        self._elapsed_steps += 1
        self.do_simulation(action, self.frame_skip)
        xyposafter = self.get_body_com("torso")[:2]

        goal_reward = -np.sum(np.abs(xyposafter - self._target_position)) + 4.0
        survive_reward = 0.05

        ctrl_cost = 0.5 * 1e-2 * np.sum(np.square(action / self.action_scaling))
        contact_cost = (
            0.5 * 1e-3 * np.sum(np.square(np.clip(self.sim.data.cfrc_ext, -1, 1)))
        )

        observation = self._get_obs()
        reward = goal_reward - ctrl_cost - contact_cost + survive_reward
        state = self.state_vector()

        done = not (np.isfinite(state).all() and 0.2 <= state[2] <= 1.0)

        infos = dict(
            reward_goal=goal_reward,
            reward_ctrl=-ctrl_cost,
            reward_contact=-contact_cost,
            reward_survive=survive_reward,
            task=self._target_position,
        )

        time_exceeded = self.elapsed_steps == self.max_episode_steps

        return observation, reward, (done or time_exceeded), infos

    def sample_task(self) -> None:
        """
        Sample a target position for the task

        Returns:
            None
        """
        self._target_position = self.np_random.uniform(
            self._min_position, self._max_position, size=2
        )
        self._elapsed_steps = 0

    def reset(self, seed = None) -> np.ndarray:
        """
        Reset the environment to the start state.

        Returns:
            np.ndarray
        """
        self._elapsed_steps = 0

        return BaseAntEnv.reset(self)

    def elapsed_steps(self) -> int:
        """
        Return the elapsed steps.

        Returns:
            int
        """
        return self._elapsed_steps

    def max_episode_steps(self) -> int:
        """
        Return the maximum episode steps.

        Returns:
            int
        """
        return self._max_episode_steps
